{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import recmetrics as rec\n",
    "import tqdm as tqdm\n",
    "import unidecode\n",
    "import joblib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "import preprocessing.sale_preprocessing as sales_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNNS_FOR_ML = ['log_delta','log_followers','conversion',\n",
    "                   'log_revenue','log_brand_appearance','log_avg_price',\n",
    "                   'artisanal', 'b_corporation', 'bio', 'biodegradable',\n",
    "                   'cadeau_ideal', 'concept_original', 'durable',\n",
    "                   'eco_friendly','excellent_sur_yuka', 'exclusivite_choose',\n",
    "                   'fabrication_a_la_demande', 'fait_main', 'gluten_free',\n",
    "                   'iconique', 'inclusive', 'innovation', 'made_in_europe',\n",
    "                   'made_in_france', 'madeinjapan', 'naturel', 'oeko_tex',\n",
    "                   'premium', 'recyclable', 'saint_valentin', 'savoir_faire',\n",
    "                   'seconde_main', 'socialement_engagee', 'serie_limitee',\n",
    "                   'tendance', 'upcycling', 'vegan', 'vintage', 'zerodechet',\n",
    "                   'category_sale',\n",
    "                   'log_monetary', 'log_frequency','log_recency',\n",
    "                   'category_1','category_2', 'category_3'\n",
    "                   ]\n",
    "\n",
    "COLS_FOR_USERS = ['user_key','start_at', 'log_monetary', 'log_frequency', 'log_recency',\n",
    "                  'category_1', 'category_2','category_3', 'log_delta']\n",
    "\n",
    "COLS_FOR_SALES = ['sale_id', 'start_at', 'log_followers','conversion',\n",
    "                   'log_revenue','log_brand_appearance','log_avg_price',\n",
    "                   'artisanal', 'b_corporation', 'bio', 'biodegradable',\n",
    "                   'cadeau_ideal', 'concept_original', 'durable',\n",
    "                   'eco_friendly','excellent_sur_yuka', 'exclusivite_choose',\n",
    "                   'fabrication_a_la_demande', 'fait_main', 'gluten_free',\n",
    "                   'iconique', 'inclusive', 'innovation', 'made_in_europe',\n",
    "                   'made_in_france', 'madeinjapan', 'naturel', 'oeko_tex',\n",
    "                   'premium', 'recyclable', 'saint_valentin', 'savoir_faire',\n",
    "                   'seconde_main', 'socialement_engagee', 'serie_limitee',\n",
    "                   'tendance', 'upcycling', 'vegan', 'vintage', 'zerodechet',\n",
    "                   'category_sale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_train_data = pd.read_csv('training_preparation/preped_train_data.csv', index_col=0).dropna().reset_index(drop=True)\n",
    "scored_test_data = pd.read_csv('training_preparation/preped_test_data.csv', index_col=0).dropna().reset_index(drop=True)\n",
    "\n",
    "first_orders = pd.read_json('training_preparation/first_order_date.json')\n",
    "first_orders['min_start_date'] = pd.to_datetime(first_orders['min_start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = '00FGOyepkGazURwZ3nrvjTXsClS2'\n",
    "raw_sales = pd.read_json('training_preparation/sales_mai22_mai23.json').dropna().reset_index(drop=True)\n",
    "\n",
    "dated_test_data = scored_test_data.merge(raw_sales[['start_at','sale_id']], on='sale_id', how ='left')\n",
    "dated_test_data['start_at'] = pd.to_datetime(dated_test_data['start_at'])\n",
    "\n",
    "\n",
    "user_information = dated_test_data[COLS_FOR_USERS].drop_duplicates()\n",
    "all_sales = dated_test_data[COLS_FOR_SALES].drop_duplicates()\n",
    "\n",
    "#pickle.load(open('TF_model.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('TF_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_entry_data(user_id, available_sales: pd.DataFrame, user_information: pd.DataFrame, df_first_order:pd.DataFrame):\n",
    "\n",
    "    # modifying user_info to have cold start if the first date is in the test set\n",
    "    cold_start_date = df_first_order[df_first_order['user_key']==user_id]['min_start_date'].iloc[0]\n",
    "    user_info = user_information[user_information['user_key']==user_id][COLS_FOR_USERS].drop_duplicates()\n",
    "    user_info.loc[user_info['start_at'] == cold_start_date, ['monetary', 'recency', 'frequency']] = 0\n",
    "    user_info.loc[user_info['start_at'] == cold_start_date, ['category_1', 'category_2', 'category_3']] = 0\n",
    "\n",
    "    # Add log_delta and merge overall dataframe\n",
    "    X = available_sales.merge(user_info, on='start_at', how='left')\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(df_for_prediction: pd.DataFrame, model):\n",
    "    \"\"\" \n",
    "    The aim of this function is to score and rank the given sales using the model. \n",
    "\n",
    "    :param df_for_prediction: containing the sale and user information \n",
    "    :param model: model used for the scoring\n",
    "    \"\"\"\n",
    "    # dataframe created to store the sales and their score\n",
    "    predicted = pd.DataFrame(df_for_prediction['sale_id'])\n",
    "    # predictions and storing\n",
    "    predicted_score = model.predict(df_for_prediction[COLUMNNS_FOR_ML].values)\n",
    "    predicted['score'] = [i for i in predicted_score.flatten()]\n",
    "\n",
    "    # sorting the scales by score with the best ones at the top\n",
    "    ordered_sales = predicted.sort_values('score',  ascending=False)\n",
    "    \n",
    "    return ordered_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preditions(user_id, available_sales, model, scored_data):\n",
    "    \"\"\" \n",
    "    The aim of this function is to predict the ranking of given sales \n",
    "    for a specific user using a given model. \n",
    "\n",
    "    :param user_id: a string containing the user id to predict for\n",
    "    :param available_sales: pd.DataFrame containing the information of the sales \n",
    "    to rank\n",
    "    :param model: model used for the scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    X = creating_entry_data(user_id, available_sales, scored_data, first_orders)\n",
    "    prediction = ranking(X, model)\n",
    "    prediction = prediction.reset_index(drop=True)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_and_truth_for_user(user_id, available_sales, model, scored_data):\n",
    "    \n",
    "    \"\"\" \n",
    "    The aim of this function is to retrun the predicted ranking using a given model\n",
    "    and real interactions for a specific user. The prediction is done on all the \n",
    "    sales available on the days the user had an interaction.\n",
    "\n",
    "    :param user_id: a string containing the user id to predict for\n",
    "    :param available_sales: pd.DataFrame containing the information of the sales \n",
    "    to rank. For each user, it is filtered on the dates where he had interactions.\n",
    "    :param model: model used for the scoring\n",
    "    :param scored_train_data: pd.DataFrame containing the known interactions of users\n",
    "    \"\"\"\n",
    "\n",
    "    #selectionner les dates de start_at sur lequel l'utilisateur aurait pu acheter\n",
    "    available_dates =  scored_data['start_at'][(scored_data['user_key'] == user_id ) & (scored_data['interaction'] ==  1)].tolist()\n",
    "    \n",
    "    # selectionner les ventes associées à ces dates\n",
    "    user_available_sales = pd.DataFrame(available_sales[available_sales['start_at'].isin(available_dates)])    \n",
    "    prediction_and_truth = make_preditions(user_id, user_available_sales, model, scored_data)\n",
    "    # truth = scored_data['sale_id'][(scored_data['user_key'] == user_id ) & (scored_data['interaction'] ==  1)]\n",
    "\n",
    "    return prediction_and_truth['sale_id']#, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(predicted, actual):\n",
    "    prec = [value for value in predicted if value in actual]\n",
    "    prec = float(len(prec)) / float(len(predicted))\n",
    "    return prec\n",
    "\n",
    "def apk(actual: list, predicted: list, k=10) -> float:\n",
    "    if not predicted or not actual:\n",
    "        return 0.0\n",
    "    \n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    true_positives = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            max_ix = min(i + 1, len(predicted))\n",
    "            score += precision(predicted[:max_ix], actual)\n",
    "            true_positives += 1\n",
    "    \n",
    "    if score == 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    return score / true_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(scored_data, all_sales, model, k):\n",
    "\n",
    "    available_users = np.array(scored_data['user_key'][scored_data['interaction'] ==  1].unique())\n",
    "\n",
    "    truth = [scored_data['sale_id'][(scored_data['user_key'] == user) & (scored_data['interaction'] ==  1)].tolist() for user in tqdm.tqdm(available_users)]\n",
    "    pred = [predicted_and_truth_for_user(user, all_sales, model, scored_data).tolist() for user in tqdm.tqdm(available_users)]\n",
    "    \n",
    "    perf = np.mean([apk(a,p,k) for a,p in zip(truth, pred)])\n",
    "    return perf, truth, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.device\n",
    "performance(dated_test_data, all_sales, model, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
